{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHicP7MazQfb5tHUY5uJri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatilAmartyaJ/NewsMiner/blob/main/Newspaper_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getUrl(location,date_obj):\n",
        "   url='https://epaper.lokmat.com/main-editions/'+location\n",
        "   url+=\"%20Main/\"\n",
        "   url+= str(date_obj)\n",
        "   url+=\"/\"\n",
        "   url+=\"1\"\n",
        "   print(url)\n",
        "   return url\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c4yPjb5rwD9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "def getArticles(url):\n",
        "    # Setup Chrome in headless mode\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "\n",
        "\n",
        "        # Wait until the element with debug-id=\"slide-0\" is present\n",
        "        wait = WebDriverWait(driver, 1000)\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[debug-id=\"slide-0\"]')))\n",
        "\n",
        "        # Locate the element and print its text\n",
        "        container = driver.find_element(By.CSS_SELECTOR, '[debug-id=\"slide-0\"]')\n",
        "        links = container.find_elements(By.TAG_NAME, 'a')\n",
        "        return len(links)\n",
        "\n",
        "    except (ConnectionRefusedError, WebDriverException) as e:\n",
        "\n",
        "            if 'driver' in locals():\n",
        "                driver.quit()\n",
        "            # time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n"
      ],
      "metadata": {
        "id": "1-9j8UgRKR-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "# Set up Chrome options without user data dir\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
        "options.add_argument(\"--disable-gpu\")  # Recommended for headless\n",
        "options.add_argument(\"--no-sandbox\")  # Good for Linux\n",
        "options.add_argument(\"--disable-dev-shm-usage\")  # Prevent resource issues\n",
        "\n",
        "# Make absolutely sure we're NOT reusing a user profile\n",
        "# (NO --user-data-dir is added here)\n",
        "\n",
        "# Create driver\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "def getPageCount(url):\n",
        "    try:\n",
        "    # Target URL\n",
        "      driver.get(url)\n",
        "\n",
        "    # Wait for the <ul> element to have content\n",
        "      wait = WebDriverWait(driver, 1000)\n",
        "      wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"ul#mTS_1_container li\")))\n",
        "\n",
        "    # Extract inner HTML\n",
        "      container = driver.find_element(By.ID, \"mTS_1_container\")\n",
        "      html_content = container.get_attribute(\"innerHTML\")\n",
        "\n",
        "      soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all <li> elements\n",
        "      li_elements = soup.find_all('li')\n",
        "      print(len(li_elements))\n",
        "      return len(li_elements)\n",
        "\n",
        "    finally:\n",
        "      driver.quit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pc1oACaGKN3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import signal\n",
        "\n",
        "def kill_chrome_processes():\n",
        "    try:\n",
        "        # Kill any existing Chrome processes\n",
        "        os.system(\"pkill -f chrome\")\n",
        "        os.system(\"pkill -f chromedriver\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Call this before starting your scraping\n",
        "kill_chrome_processes()"
      ],
      "metadata": {
        "id": "_L3OIEl2AXZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "# Image URL (replace with your target image)\n",
        "location_dict={\n",
        "    \"Kolhapur\":\"KOLK\",\n",
        "    \"Mumbai\": \"MULK\",\n",
        "    \"Pune\":\"PUNK\",\n",
        "    \"Nagpur\":\"NPLK\",\n",
        "    \"Nashik\":\"NSLK\",\n",
        "    \"Chhatrapati Sambhajinagar\":\"AULK\",\n",
        "    \"Jalgoan\":\"JLLK\",\n",
        "    \"Akola\":\"AKLK\",\n",
        "    \"Solapur\":\"SOLK\",\n",
        "    \"Ahilyanagar\":\"ANLK\",\n",
        "    \"Goa\":\"GALK\",\n",
        "    \"New Delhi\":\"DLLK\"\n",
        "}\n",
        "print(\"Valid locations are: \")\n",
        "for key in location_dict.keys():\n",
        "    print(key)\n",
        "location=input(\"Enter the Location: \")\n",
        "\n",
        "\n",
        "date_str = input(\"Enter a date (YYYY-MM-DD): \")\n",
        "date_obj = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n",
        "\n",
        "print(\"You entered:\", date_obj)\n",
        "\n",
        "invalid_dates=[\"2025-01-27\",\"2025-03-14\",\"2025-05-02\",\"2025-08-16\"]\n",
        "url_prefix = 'https://epaper.lokmat.com/'\n",
        "if date_obj.strftime(\"%Y-%m-%d\") not in invalid_dates and date_obj<=datetime.now().date():\n",
        "   url= getUrl(location,date_obj)\n",
        "   number_of_pages=getPageCount(url)\n",
        "   clean_url = url.rsplit('/', 1)[0]\n",
        "   print(clean_url)\n",
        "   for i in range(1,number_of_pages+1):\n",
        "       url_page=clean_url+\"/\"+str(i)\n",
        "       print(url_page)\n",
        "       num_articles=getArticles(url_page)\n",
        "       print(\"number of articles for page number \",i,\" are \",num_articles)\n",
        "       for j in range(1,num_articles+1):\n",
        "          article_url=url_prefix+\"articlepage.php?articleid=LOK_\"\n",
        "          article_url+=location_dict[location]+\"_\"\n",
        "          article_url+=  date_obj.strftime(\"%Y%m%d\")\n",
        "          article_url+=\"_\"+str(i)+\"_\"+str(j)\n",
        "          print(article_url)\n",
        "\n",
        "\n",
        "\n",
        "# https://epaper.lokmat.com/articlepage.php?articleid=LOK_KOLK_20250828\n",
        "# 'KOLK/2025/08/24/ArticleImages/68aa3de90ef42.jpg'\n",
        "\n",
        "# # Extract the filename from the URL\n",
        "# filename = os.path.basename(image_url)\n",
        "\n",
        "# # Make the request to get the image content\n",
        "# response = requests.get(image_url)\n",
        "\n",
        "# if response.status_code == 200:\n",
        "#     with open(filename, 'wb') as f:\n",
        "#         f.write(response.content)\n",
        "#     print(f\"Image saved as: {filename}\")\n",
        "# else:\n",
        "#     print(f\"Failed to download image. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "rGfiwL6D1bDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}